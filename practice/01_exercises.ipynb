{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7840a8-cb4a-4c7c-8850-b9c882f2cc38",
   "metadata": {},
   "source": [
    "# EXERCISES FOR: 01. Neural Network Regression with TensorFlow\n",
    "\n",
    "## ðŸ›  Exercises\n",
    "\n",
    "1. Create your own regression dataset (or make the one we created in \"Create data to view and fit\" bigger) and build fit a model to it.\n",
    "2. Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform?\n",
    "3. Try and improve the results we got on the insurance dataset, some things you might want to try include:\n",
    "  * Building a larger model (how does one with 4 dense layers go?).\n",
    "  * Increasing the number of units in each layer.\n",
    "  * Lookup the documentation of [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) and find out what the first parameter is, what happens if you increase it by 10x?\n",
    "  * What happens if you train for longer (say 300 epochs instead of 200)? \n",
    "4. Import the [Boston pricing dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data) from TensorFlow [`tf.keras.datasets`](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) and model it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0487fbf-1a42-4af8-8e2d-a94eec71d180",
   "metadata": {},
   "source": [
    "### Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348c437-35b7-4d39-b39d-86a0193ca8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamp\n",
    "import datetime\n",
    "\n",
    "print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50021b-7bf6-4314-9521-9f8de69ad231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "print(tf.__version__) # find the version number (should be 2.x+)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e0387-f02c-4afb-aec4-39f4cc4519e7",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "#### [1] Create humidity and pressure dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d7896-8fbb-48f5-ba92-6c0987168b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Time vector, in days:\n",
    "time_max = 1000 # days\n",
    "time = tf.constant(np.arange(0,time_max,0.5),dtype=tf.float32) # days\n",
    "\n",
    "# Build the humidity function, in arb. units:\n",
    "def build_hum(time,time_max,base,lin,cub,exp,per,ran):\n",
    "    np.random.seed(0)\n",
    "    hum_lin = time*lin/time_max # Linear change\n",
    "    hum_cub = ((time-time_max/2)/time_max)**3*cub # Cubic change\n",
    "    hum_exp = -np.exp(time/time_max)*exp # Exponential change\n",
    "    hum_per = np.sin(time/time_max*12*np.pi)*per # Periodical change\n",
    "    hum_ran = np.random.random(len(time))*ran-ran/2 # Random change\n",
    "    hum = base + hum_lin + hum_cub + hum_exp + hum_per + hum_ran\n",
    "    return tf.constant(hum,dtype=tf.float32)\n",
    "\n",
    "base_h = 1\n",
    "lin_h = 1\n",
    "cub_h = 2\n",
    "exp_h = 1\n",
    "per_h = 1\n",
    "ran_h = 0\n",
    "\n",
    "humidity = build_hum(time,time_max,base_h,lin_h,cub_h,exp_h,per_h,ran_h) # [%]\n",
    "\n",
    "# Build the pressure function, in arb. units:\n",
    "def build_pre(time,time_max,base,lin,ran):\n",
    "    np.random.seed(0)\n",
    "    pres = base+time/time_max*lin + np.random.random(len(time))*ran-ran/2\n",
    "    return tf.constant(pres,dtype=tf.float32)\n",
    "\n",
    "base_p = 1\n",
    "lin_p = 3\n",
    "ran_p = 0.1\n",
    "pressure = build_pre(time,time_max,base_p,lin_p,ran_p)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot:\n",
    "plt.plot(time,humidity,'-o',label='Humidity',alpha=0.7)\n",
    "plt.plot(time,pressure,'-s',label='Pressure',alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Training dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792548f0-d16b-4c74-85aa-de6d6ab13ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test datasets:\n",
    "time_test = tf.constant(np.random.random(100)*time_max,dtype=tf.float32)\n",
    "humidity_test = build_hum(time_test,time_max,base_h,lin_h,cub_h,exp_h,per_h,ran_h) # [%]\n",
    "pressure_test = build_pre(time_test,time_max,base_p,lin_p,ran_p)\n",
    "\n",
    "# Plot:\n",
    "plt.plot(time_test,humidity_test,'o',label='Humidity',alpha=0.7)\n",
    "plt.plot(time_test,pressure_test,'s',label='Pressure',alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Testing dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c17b07-dc4d-415f-b0a2-07019c6c089d",
   "metadata": {},
   "source": [
    "#### [1,2,3] Build ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04603860-0c44-47ba-bc0c-9a288e45f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a model using the Sequential API\n",
    "model_pres = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Dense(20,activation='relu'),\n",
    "    tf.keras.layers.Dense(1, input_shape=[1])\n",
    "])\n",
    "\n",
    "model_hum = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512,activation='relu', input_shape=[1]),\n",
    "    tf.keras.layers.Dense(512,activation='relu'),\n",
    "    tf.keras.layers.Dense(256,activation='relu'),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def compile_train_model(model,loss,optimizer,metrics,X_train,Y_train,epochs,exp_dims=True):\n",
    "    # Compile the model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=metrics)\n",
    "    # Train the model:\n",
    "    if exp_dims:\n",
    "        history = model.fit(tf.expand_dims(X_train, axis=-1), Y_train, epochs=epochs, verbose=0)\n",
    "    else:\n",
    "        history = model.fit(X_train, Y_train, epochs=epochs, verbose=0)\n",
    "\n",
    "    return history\n",
    "\n",
    "# Prepare models:\n",
    "X_train = time\n",
    "# Humidity model:\n",
    "history_hum = compile_train_model(model_hum,\n",
    "                    tf.keras.losses.mae, # Loss function\n",
    "                    tf.keras.optimizers.Adam(learning_rate=0.01), # Optimizer\n",
    "                    [\"mae\"], # Metrics\n",
    "                    X_train,humidity, # Data\n",
    "                    20) # Epochs\n",
    "# Check the model summary\n",
    "model_hum.summary()\n",
    "# Plot training history:\n",
    "pd.DataFrame(history_hum.history).plot()\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\"); # note: epochs will only show 100 since we overrid the history variable\n",
    "plt.yscale('log');\n",
    "\n",
    "# Pressure model: \n",
    "history_pres = compile_train_model(model_pres,\n",
    "                    tf.keras.losses.mae,\n",
    "                    tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                    [\"mae\"],\n",
    "                    X_train,pressure,\n",
    "                    30)\n",
    "# Check the model summary\n",
    "model_pres.summary()\n",
    "# Plot training history:\n",
    "pd.DataFrame(history_pres.history).plot()\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\") # note: epochs will only show 100 since we overrid the history variable\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f8f84-626e-4aac-8fa4-3af3513004fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model_hum, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558abce-1032-4242-9af8-c28ccaf937a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_evaluate_plot(model,X_test,Y_test,title=None):\n",
    "    # Predict:\n",
    "    Y_pred = model.predict(X_test)\n",
    "    # Plot predictions and ground truth:\n",
    "    plt.plot(X_test,Y_test,'s',label='Ground truth',color='green',alpha=0.7)\n",
    "    plt.plot(X_test,Y_pred,'d',label='Predictions',color='red',alpha=0.7)\n",
    "    plt.legend(title=title)\n",
    "    plt.title('Predictions in Testing dataset')\n",
    "    plt.show()\n",
    "\n",
    "# Pressure model:\n",
    "predict_evaluate_plot(model_pres,time_test,pressure_test,title='Pressure')\n",
    "# Humidity model:\n",
    "predict_evaluate_plot(model_hum,time_test,humidity_test,title='Humidity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab0e0c-a32b-4c13-9763-8fdab9bd25d1",
   "metadata": {},
   "source": [
    "#### [4] Work with the Boston house pricing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90dfa9-4b04-4ab8-86fa-eabe2c59aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
    "    path='boston_housing.npz', test_split=0.2, seed=113\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efdac8a-720b-42eb-b7ac-c04e57657ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape:\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364078c5-6c7f-4e79-8a43-ba58500e5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors:\n",
    "x_train_tf = tf.constant(x_train)\n",
    "y_train_tf = tf.constant(y_train)\n",
    "x_test_tf = tf.constant(x_test)\n",
    "y_test_tf = tf.constant(y_test)\n",
    "# Check data shape:\n",
    "x_train_tf.shape, y_train_tf.shape, x_test_tf.shape, y_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97371b-00a2-4861-bde4-c10ea50ad70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models:\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a model using the Sequential API\n",
    "model_boston = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512,activation='relu', input_shape=x_train_tf[0].shape),\n",
    "    tf.keras.layers.Dense(256,activation='relu'),\n",
    "    tf.keras.layers.Dense(256,activation='relu'),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "history = compile_train_model(model_boston,\n",
    "                    tf.keras.losses.mse, # Loss function\n",
    "                    tf.keras.optimizers.Adam(learning_rate=0.003), # Optimizer\n",
    "                    [\"mae\"], # Metrics\n",
    "                    x_train_tf,y_train_tf, # Data\n",
    "                    200, # Epochs\n",
    "                    exp_dims=False) \n",
    "# Plot training history:\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\") # note: epochs will only show 100 since we overrid the history variable\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa6d2ed-01b8-4621-aa5a-135eb7e33f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict:\n",
    "y_pred = model_boston.predict(x_test_tf,verbose=0).squeeze()\n",
    "#Evaluate:\n",
    "mae = tf.reduce_mean(tf.abs(y_test_tf-y_pred.squeeze())).numpy()\n",
    "print('Mean Absolute Error:', mae)\n",
    "# Plot predictions and ground truth:\n",
    "plt.plot(y_test_tf,y_pred,'o',label='Comparison',color='green',alpha=0.7)\n",
    "plt.plot(y_test_tf,y_test_tf,'-',label='Ideal',alpha=0.8)\n",
    "#plt.plot(y_pred,'d',label='Predictions',color='red',alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xlabel('Ground truth'), plt.ylabel('Predictions')\n",
    "plt.title('Predictions in Testing dataset');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7202874-f312-4400-acce-8557fc10dc15",
   "metadata": {},
   "source": [
    "## ðŸ“– Extra curriculum\n",
    "\n",
    "If you're looking for extra materials relating to this notebook, I'd check out the following:\n",
    "\n",
    "* [MIT introduction deep learning lecture 1](https://youtu.be/njKP3FqW3Sk) - gives a great overview of what's happening behind all of the code we're running.\n",
    "* Reading: 1-hour of [Chapter 1 of Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html) by Michael Nielson - a great in-depth and hands-on example of the intuition behind neural networks.\n",
    "\n",
    "To practice your regression modelling with TensorFlow, I'd also encourage you to look through [Lion Bridge's collection of datasets](https://lionbridge.ai/datasets/) or [Kaggle's datasets](https://www.kaggle.com/data), find a regression dataset which sparks your interest and try to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab01b47-1601-4026-b87b-d32991d397ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
